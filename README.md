#  Evaluating Source Accuracy in LLMs Across Domains

This project evaluates the accuracy of sources cited by Large Language Models (LLMs). It does so by prompting multiple web search-enabled LLMs (GPT-5, Claude, Perplexity) with a predetermined set of questions from five domains, covering both academic and non-academic topics. The questions are carefully curated to include a mix of niche and broad topics to assess how LLM performance varies when online information is limited.

LLM responses are then evaluated for both factual correctness and the reliability of cited sources through human annotation. Multiple prompting strategies such as direct, precise, verification, and in-context learning (ICL) are used to observe their impact on response accuracy and citation quality.
